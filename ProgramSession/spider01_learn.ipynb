{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 什么是爬虫\n",
    "\n",
    "**爬虫：** 网络爬虫，其实叫作网络数据采集更容易理解。\n",
    "\n",
    "> 通过编程向网络服务器请求数据（HTML表单），然后解析HTML，提取出自己想要的数据\n",
    "\n",
    "![](spider01.jpg)\n",
    "\n",
    "归纳为四大步：\n",
    "\n",
    "- 根据url获取HTML数据\n",
    "- 解析HTML，获取目标信息\n",
    "- 存储数据\n",
    "- 重复第一步\n",
    "\n",
    "这会涉及到数据库、网络服务器、HTTP协议、HTML、数据科学、网络安全、图像处理等非常多的内容。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 什么是HTML\n",
    "\n",
    "HTML是英文Hyper Text Mark-up Language **(超文本标记语言)** 的缩写，它规定了自己的语法规则，用来表示比\"文本\"更丰富的意义，比如图片，表格，链接等。浏览器理解HTML语言的语法，可以用来查看HTML文档。目前互联网上的绝大部分网页都是使用HTML编写的。\n",
    "\n",
    "总结一下，HTML是一种用于创建网页的标记语言，里面嵌入了文本、图像等数据，可以被浏览器读取，并渲染成我们看到的网页样子。因此我们需要先 **爬取HTML**，再 **解析数据**，因为数据藏在HTML里。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 一个HTML文档的例子：\n",
    "\n",
    "HTML语法的本质是**给文本加上表明文本含义的标签(Tag)**，让用户（人或程序）能对文本得到更好的理解。HTML文档由嵌套的HTML元素构成，它们用HTML标签表示，HTML标签包含于尖括号中。在一般情况下，一个元素由一对标签表示：“开始标签”<...>与“结束标签”</...>。元素如果含有文本内容，就被放置在这些标签之间。\n",
    "\n",
    "\n",
    "```\n",
    "<html>\n",
    "   <head>\n",
    "     <meta charset=\"UTF-8\">\n",
    "     <title>页面标题</title>\n",
    "   </head>\n",
    "   <body>\n",
    "     欢迎访问<a href=\"http://www.baidu.com/\">百度</a>！\n",
    "     <h1>我的第一个标题</h1>\n",
    "     <p>我的第一个段落。</p>\n",
    "   </body>\n",
    "</html> \n",
    "\n",
    "所有的HTML文档都应该有一个<html>标签，<html>标签可以包含两个部分:<head>和<body>。\n",
    "\n",
    "<head>标签用于包含整个文档的一般信息，比如文档的标题（<title>标签用于包含标题），对整个文档的描述，文档的关键字等等。文档的具体内容就要放在<body>标签里了。\n",
    "\n",
    "<a>标签用于表示链接，在浏览器中查看HTML文档时，点击<a>标签括起来的内容时，通常会跳转到另一个页面。这个要跳转到的页面的地址由<a>标签的href属性指定。上面的<a href=\"http:///www.baidu.com/\">中，href属性的值就是http://www.baidu.com/。\n",
    "\n",
    "```\n",
    "\n",
    "通过不同的标签，HTML文档可以包含不同的内容，比如文本，链接，图片，列表，表格，表单，框架等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HTML格式详细说明\n",
    "\n",
    "标签本质上是对它所包含的内容的说明，可能会有属性，来给出更多的信息。比如\\<img>（图片）标签有src属性（用于指明图片的地址），width和height属性（用于说明图片的宽度和高度）。  \n",
    "HTML里能使用哪些标签，这些标签分别可以拥有哪些属性，这些都是有规定的，学习HTML其实也就是学习这些标签了。后面我们会对常用的HTML标签做出简短的介绍。\n",
    "\n",
    "标签通常有开始部分和结束部分（也被称为**开始标签**和**结束标签**），它们一起限定了这个标签所包含的内容。**属性只能在开始标签中指定，属性值可以用单引号或双引号括起来。结束标签都以/加上标签名来表示。**  \n",
    "有时候，有些标签并不包含其它内容（只包括自己的属性，甚至连属性都没有），这种情况下，可以写成类似这样：\\<img src=\"logo.gif\" />。**注意最后的一个空格和一个反斜杠，它说明这个标签已经结束，不需要单独的结束标签了。**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**常见标签列表：**\n",
    "\n",
    "|标签|描述|\n",
    "|--|--|\n",
    "|<!DOCTYPE>| 告知 Web 浏览器页面使用了哪种 HTML 版本|\n",
    "|\\<html>| 告知浏览器这是一个 HTML 文档，是所有其他 HTML 元素（除了 <!DOCTYPE> 标签）的容器|\n",
    "|\\<head>| 是所有头部元素的容器，必须包含文档的标题（title），可以包含脚本、样式、meta 信息以及其他更多的信息。<br> 如在\\<head>内指定 \\<meta charset=\"utf-8\"> 表示定义网页编码格式为 utf-8|\n",
    "|\\<title>| 定义文档的标题|\n",
    "|\\<body> | 定义文档的主体|\n",
    "|\\<h1> to \\<h6> |定义重要等级不同的 HTML 标题|\n",
    "|\\<p> | 定义段落|\n",
    "|\\<br>| 表示插入一个换行符，没有结束标签|\n",
    "|\\<hr>| 显示一条水平线用于分隔内容|\n",
    "|\\<table> | 定义表格|\n",
    "|\\<a href=\"\"> | 定义超链接， href 属性指定链接的目标|\n",
    "|\\<img src=\"\" alt=\"\" /> | 定义图像，src属性规定显示图像的 URL，alt属性规定图像的替代文本|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HTML（超文本标记语言）是Web的最基本构建模块，它定义了Web内容的含义和结构。除HTML外，其他技术通常用于描述网页的外观/样式（CSS）或功能/行为（JavaScript）。\n",
    "- **CSS(层叠样式表)** 定义如何显示 HTML 元素，解决了内容与表现分离的问题，通过将样式存储在外部样式表中，可以极大地提高工作效率。\n",
    "\n",
    "- **JavaScript**是一种脚本语言，通常被直接嵌入 HTML 页面，被设计用来向 HTML 页面添加各种交互行为。\n",
    "\n",
    "HTML 定义了网页的内容，CSS 描述了网页的布局，JavaScript 控制了网页的行为。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 爬虫的合法性\n",
    "\n",
    "**爬虫**：一段自动抓取互联网信息的程序，从互联网上抓取对于我们有价值的信息。\n",
    "\n",
    "网络爬虫的尺寸：\n",
    "\n",
    "- 小规模，数据量小，爬取速度不敏感（Requests库） >90% ——爬取网页\n",
    "- 中规模，数据量较大，爬取速度较敏感（Scrapy库）——爬取网站\n",
    "- 大规模，搜索引擎，爬取速度关键，定制开发 ——爬取全网\n",
    "\n",
    "网络爬虫的问题：\n",
    "- 性能骚扰：Web服务器默认接受人类访问，受限于编写水平和目的，网络爬虫将会为其带来巨大的资源开销。\n",
    "- 法律风险：数据产权归属、隐私泄露等。\n",
    "\n",
    "**网络爬虫的限制**：\n",
    "\n",
    "- 来源审查：判断User‐Agent进行限制  \n",
    "检查来访HTTP协议头的User‐Agent域，只响应浏览器或友好爬虫的访问\n",
    "- 发布公告：**Robots协议**  \n",
    "告知所有爬虫网站的爬取策略，要求爬虫遵守\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Robots协议\n",
    "\n",
    "Robots Exclusion Standard，网络爬虫排除标准\n",
    "- 作用：网站告知网络爬虫哪些页面可以抓取，哪些不行\n",
    "- 形式：在网站根目录下的robots.txt文件\n",
    "\n",
    "对于没有设定 robots.txt 的网站，可以通过网络爬虫获取没有口令加密的数据，也就是该网站所有页面数据都可以爬取。  \n",
    "如果网站有 robots.txt 文档，就要判断是否有禁止访客获取的数据。\n",
    "\n",
    "**案例：雪球的Robots协议**\n",
    "\n",
    "https://xueqiu.com/robots.txt\n",
    "\n",
    "Robots协议基本语法：(*代表所有，/代表根目录)\n",
    "- User‐agent: *\n",
    "- Disallow: /  \n",
    "\n",
    "语法示例：  \n",
    "User-agent: * 代表所有的网络爬虫来源（ * 表示通配符 ）  \n",
    "Disallow: /ABC/ 这里定义是禁止爬取ABC目录下面的目录  \n",
    "Disallow: /cgi-bin/* .htm 禁止访问/cgi-bin/目录下的所有以\".htm\"为后缀的URL(包含子目录）  \n",
    "Disallow: /*?* 禁止访问网站中所有包含问号 (?) 的网址  \n",
    "Disallow: /.jpg$ 禁止抓取网页所有的.jpg格式的图片  \n",
    "Disallow:/ab/adc.html 禁止爬取ab文件夹下面的adc.html文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 爬虫的基本原理\n",
    "网页请求的过程分为两个环节：\n",
    "- Request （请求）：每一个展示在用户面前的网页都必须经过这一步，也就是向服务器发送访问请求。\n",
    "- Response（响应）：服务器在接收到用户的请求后，会验证请求的有效性，然后向用户（客户端）发送响应的内容，客户端接收服务器响应的内容，将内容展示出来，就是我们所熟悉的网页。\n",
    "\n",
    "网页请求的方式：\n",
    "- GET：最常见的方式，一般用于获取或者查询资源信息，也是大多数网站使用的方式，响应速度快。\n",
    "- POST：相比 GET 方式，多了以表单形式上传参数的功能，因此除查询信息外，还可以修改信息。\n",
    "\n",
    "所以，在写爬虫前要先确定向谁发送请求，用什么方式发送。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### requests第三方库\n",
    "\n",
    "The requests module allows you to send HTTP requests using Python.  \n",
    "The HTTP request returns a `Response Object` with all the response data (content, encoding, status, etc).  \n",
    "Detail: https://requests.readthedocs.io/en/master/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### requests 库\n",
    ">安装：pip install requests  \n",
    ">调用：import requests\n",
    "\n",
    "#### requests库的7种方法\n",
    "```\n",
    "requests.requests()   构造一个请求，支撑以下各方法的基础方法\n",
    "requests.get()        获取HTML网页的主要方法\n",
    "requests.head()       获取HTML网页头部信息的方法\n",
    "requests.post()       向HTML网页提交POST请求\n",
    "requests.put()        向HTML网页提交PUT请求的方法\n",
    "requests.patch()      向HTML网页提交局部修改请求\n",
    "requests.delete()     向HTML页面提交删除请求\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**get方法：** `r = requests.get(url, params=None, kwargs)`\n",
    "```\n",
    "url: 拟获取页面的url链接；\n",
    "params：url中的额外参数，字典或字节流格式，可选；\n",
    "kwargs：12个控制访问的参数\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reponse对象的属性：**\n",
    "```\n",
    "r.status_code          HTTP请求的返回状态，200表示成功，404表示失败\n",
    "r.text                 HTTP响应内容的字符串形式，即，url对应的页面内容\n",
    "r.encoding             从HTTP header中猜测的响应内容编码方式\n",
    "r.apparent_encoding    从内容中分析出的响应内容编码方式（备选编码方式）\n",
    "r.content              HTTP响应内容的二进制形式\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "403\n",
      "403 Forbidden. Your IP Address: 112.10.55.241 .\n"
     ]
    }
   ],
   "source": [
    "#### 使用 GET 方式抓取数据\n",
    "\n",
    "import requests        # 导入requests包\n",
    "url = 'https://xueqiu.com/'\n",
    "res = requests.get(url)        # Get方式获取网页数据\n",
    "\n",
    "print(res.status_code)\n",
    "print(res.text)\n",
    "\n",
    "#### 查看response对象中request请求的headers属性：res.request.headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise：**\n",
    "    \n",
    "    请大家使用requests库去获取你感兴趣的网页，并查看response对象的属性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 设置headers\n",
    "\n",
    "网络爬虫的限制：检查来访HTTP协议头的User‐Agent域，只响应浏览器或友好爬虫的访问。  \n",
    "因此在请求网页爬取的时候，输出的text信息中会出现抱歉、无法访问等字眼，表示禁止爬取，此时需要通过反爬机制去解决这个问题。  \n",
    "设置headers是解决requests请求反爬的方法之一，相当于我们通过设置headers信息，模拟成是浏览器在访问网站 。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### 使用 GET 方式抓取数据\n",
    "\n",
    "import requests        # 导入requests包\n",
    "url = 'https://xueqiu.com/'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.111 Safari/537.36'}\n",
    "res = requests.get(url, headers = headers)        # Get方式获取网页数据（设置headers）\n",
    "res.encoding = 'utf-8'  # 保证中文的显示\n",
    "res.status_code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**例1**：百度搜索引擎关键词提交\n",
    "\n",
    "百度的关键词接口：http://www.baidu.com/s?wd=keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "http://www.baidu.com/s?wd=%E5%A3%81%E7%BA%B8\n",
      "710860\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "kw = {'wd':'壁纸'}\n",
    "headers = {'User-Agent':'Mozilla/5.0'}\n",
    "r = requests.get(\"http://www.baidu.com/s\",params=kw,headers=headers)\n",
    "\n",
    "print(r.status_code)\n",
    "print(r.request.url)\n",
    "print(len(r.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**例2**：网络图片的爬取和存储\n",
    "\n",
    "图片url：  \n",
    "http://image.ngchina.com.cn/2019/1023/20191023050955671.gif  \n",
    "https://ns-strategy.cdn.bcebos.com/ns-strategy/upload/fc_big_pic/part-00142-1269.jpg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = 'https://ns-strategy.cdn.bcebos.com/ns-strategy/upload/fc_big_pic/part-00142-1269.jpg'\n",
    "path = 'abc.gif'\n",
    "# path = url.split('/')[-1]\n",
    "r = requests.get(url)\n",
    "print(r.status_code)\n",
    "\n",
    "with open(path, 'wb') as f:\n",
    "    f.write(r.content)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise：**\n",
    "\n",
    "1. 请大家访问如下链接（或是你感兴趣的网站），查看它们的网页源代码和robots协议\n",
    "\n",
    ">http://news.sina.com.cn  \n",
    ">http://news.qq.com  \n",
    ">http://www.stats.gov.cn/ （无robots协议）\n",
    "\n",
    "2. 请大家通过运行上述实例熟练掌握对于requests库中get方法的使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BeautifulSoup\n",
    "\n",
    "官网地址：https://www.crummy.com/software/BeautifulSoup/\n",
    "\n",
    "BeautifulSoup 是 python 的一个第三方库，最主要的功能是用来提取 XML 和 HTML 中的数据。官方解释如下：\n",
    "\n",
    ">Beautiful Soup 提供一些简单的、python 式的函数用来处理导航、搜索、修改分析树等功能。它是一个工具箱，通过解析文档为用户提供需要抓取的数据，因为简单，所以不需要多少代码就可以写出一个完整的应用程序。 Beautiful Soup 自动将输入文档转换为 Unicode 编码，输出文档转换为 utf-8 编码。你不需要考虑编码方式，除非文档没有指定一个编码方式，这时，Beautiful Soup 就不能自动识别编码方式了。然后，你仅仅需要说明一下原始编码方式就可以了。 Beautiful Soup 已成为和 lxml、html6lib 一样出色的 python 解释器，为用户灵活地提供不同的解析策略或强劲的速度。\n",
    "\n",
    "安装方法：`pip install bs4`\n",
    "\n",
    "调用方法：`from bs4 import BeautifulSoup`\n",
    "\n",
    "BeautifulSoup 支持 Python 标准库中的 HTML 解析器，也支持一些第三方的解析器。如果我们不安装它，则 Python 会使用 Python 默认的解析器 html.parser。lxml 解析器更加强大，速度更快，推荐安装。\n",
    "\n",
    "安装方法：`pip install lxml`\n",
    "\n",
    "**一般步骤：**\n",
    "1. 通过requests库获得html页面的内容\n",
    "2. 使用BeautifulSoup库对获得的html页面进行解析\n",
    "3. 使用BeautifulSoup以及正则表达式来进一步提取我们想要的关键信息\n",
    "4. 将信息组织并输出\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 创建BeautifulSoup对象 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests        # 导入requests包\n",
    "url = 'https://xueqiu.com/'\n",
    "headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "# headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.111 Safari/537.36'}\n",
    "res = requests.get(url, headers = headers)        # Get方式获取网页数据（设置headers）\n",
    "res.encoding = 'utf-8'  # 保证中文的显示\n",
    "demo = res.text\n",
    "\n",
    "from bs4 import BeautifulSoup         # 从bs4中导入BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(demo, 'html.parser')\n",
    "print(soup.prettify())          # .prettify()方法用于为标签及字符串增加换行符，可用于标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "demo = open('html.html','r',encoding='utf-8').read()    # 打开本地的html文件（注意html文件所在的位置）\n",
    "\n",
    "soup = BeautifulSoup(demo, 'lxml')\n",
    "    \n",
    "# print(demo)\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beautiful Soup 将复杂 HTML 文档转换成一个复杂的树形结构，每个节点都是 Python 对象。因此Beautiful Soup库是解析、遍历、维护“标签树”的功能库。\n",
    "\n",
    "```\n",
    "<title>个股点评_证券_新浪财经</title>\n",
    "\n",
    "<a href=\"https://finance.sina.com.cn/stock/zqgd/2020-11-22/doc-iiznezxs3063378.shtml\" target=\"_blank\">*ST欧浦或面临退市:因公司控股股东佛山市中基投资宣告破产</a>\n",
    "```\n",
    "\n",
    "上述是两个标签的示例，利用 Beautiful Soup可以非常方便的将标签中的信息提取出来。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise：**\n",
    "\n",
    "1. 请大家获取百度首页的html内容，并使用beautifulsoup对其进行解析，将其中的图片保存到本地C盘的根目录下。\n",
    "(tips: 利用img标签获取图片)\n",
    "\n",
    "2. 思考如何批量获取标签内容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 小结：\n",
    "\n",
    "1. HTML——超文本标记语言：标签（Tag）\n",
    "\n",
    "2. 爬虫的框架、Robots协议\n",
    "\n",
    "1. 利用requests.get 方法获得html内容：通过设置headers将爬虫伪装成浏览器访问进行反爬虫\n",
    "\n",
    "2. 利用BeautifulSoup将网页解析为标签树：提取单个标签（名称、属性、NavigableString）\n",
    "\n",
    "中文官方文档：https://beautifulsoup.readthedocs.io/zh_CN/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
